# Master build management file for covid forecasting datasets.

##########
# CONFIG #
##########

# imports
import os

# set master config file with paths and globals
configfile: 'config/config.yaml'

# pull paths from ENV vars
envvars:
    'IEC',
    'IEC1',
    'TMP',
    'MBTOKEN_TILE'
    
# set paths, leveraging settings in the config file
CODE=os.path.expanduser(config['globals']['ccode'])
DATA=os.path.expanduser(config['globals']['cdata'])
TILESET_NAME=config['globals']['tileset_name']
MBTOKEN_TILE=os.environ['MBTOKEN_TILE']
TMP=os.path.expanduser(os.environ['TMP'])
IEC=os.path.expanduser(os.environ['IEC'])
IEC1=os.path.expanduser(os.environ['IEC1'])


#########
# RULES #
#########

# master rule to define the final output
rule all:
    input:
        f'{TMP}/tileset_push.log',
#        f'{TMP}/covid_data.mbtiles',
#        rules.push_pred_metadata.log,
#        rules.push_pred_data.log,
#        rules.test_merged_data.log,
        f'{TMP}/push_pred_metadata.log',
        f'{TMP}/test_merged_data.log',
        f'{TMP}/push_pred_data.log'

# pull predicted data from google cloud
# rerun_indicator gets `touch`ed from cronjob to trigger pipeline execution
rule pull_predicted_data:
    input:
        f'{TMP}/rerun_indicator.txt',
        f'{CODE}/b/pull_predicted_data.sh',
        f'{CODE}/b/pull_predicted_data_helper.py'
    conda: 'config/forecasting.yaml'
    log: f'{TMP}/data_pull.log'
    shell: f'source {CODE}/b/pull_predicted_data.sh {CODE}/predictions_credential.json {CODE}/b/pull_predicted_data_helper.py {DATA}/all_rt_estimates > {{log}}'

# create tabular data tables of predicted variables (from Anup et al), identified to pc11 dists
# UPDATE TO WILDCARD CSVS
rule process_predicted_data:
    input:
        f'{DATA}/all_rt_estimates/WB_district_Rt.csv',
        f'{TMP}/data_pull.log',
        f'{CODE}/b/process_predicted_data.do'
    output:
        f'{DATA}/pred_data_district.dta',
        f'{DATA}/pred_data_state.dta',
        f'{DATA}/pred_data_district.csv',
        f'{DATA}/pred_data_state.csv',
        f'{DATA}/pred_data_rt_choropleth.dta',
    shell: f'stata -b {CODE}/b/process_predicted_data.do '

# create data tables for all other DDL covid data to display in district popups
rule process_ddl_data:
    input:
        f'{IEC}/covid/hospitals/pc_hospitals_dist.dta',
        f'{CODE}/b/process_ddl_data.do'
    output:
        f'{DATA}/ddl_data.dta',
    shell: f'stata -b {CODE}/b/process_ddl_data.do '

# merge DDL and predicted data
rule merge_ddl_pred_data:
    input:
        rules.process_predicted_data.output,        
        rules.process_ddl_data.output,        
        f'{CODE}/b/merge_ddl_pred_data.do'
    output:
        f'{DATA}/merged_data_district.dta',
    shell: f'stata -b {CODE}/b/merge_ddl_pred_data.do '

# creation of geojson from full tabular district data (all dates X rt, etc) for the plots
rule dist_data_to_geojson:
    input:
        rules.merge_ddl_pred_data.output,        
        f'{DATA}/district-simplified-mapshaper.shp',
        f'{CODE}/b/data_to_geojson.py'
    output:
        f'{DATA}/district.geojson'
    conda: 'config/forecasting_spatial.yaml'
    shell: f'python {CODE}/b/data_to_geojson.py --intable {rules.merge_ddl_pred_data.output} --inshp {DATA}/district-simplified-mapshaper.shp --outfile {{output}}'

# creation of geojson from latest Rt observation by district, for the choropleth
rule map_data_to_geojson:
    input:
        rules.process_predicted_data.output,        
        f'{DATA}/district-simplified-mapshaper.shp',
        f'{CODE}/b/data_to_geojson.py'
    output:
        f'{DATA}/district-map.geojson'
    conda: 'config/forecasting_spatial.yaml'
    shell: f'python {CODE}/b/data_to_geojson.py --intable {DATA}/pred_data_rt_choropleth.dta --inshp {DATA}/district-simplified-mapshaper.shp --outfile {{output}}'

# test basic data quality
rule test_merged_data:
    input:
        rules.merge_ddl_pred_data.output,        
        rules.dist_data_to_geojson.output,        
        f'{CODE}/b/test_merged_data.py'
    log:
        f'{TMP}/test_merged_data.log'
    conda: 'config/forecasting_spatial.yaml'
    shell: f'python {CODE}/b/test_merged_data.py > {{log}}'

## creation of geojson from tabular district data
#rule push_predicted_metadata:
#    input:
#        rules.process_predicted_data.output,        
#        rules.test_merged_data.log,        
#    log:
#        f'{TMP}/push_pred_metadata.log',
#    conda: 'config/forecasting.yaml'
#    shell: f'python {CODE}/b/push_predicted_metadata.py --file {DATA}/pred_metadata.js > {{log}}'

# push data for public access
rule push_public_data:
    input:
        rules.merge_ddl_pred_data.output,        
        rules.test_merged_data.log,        
        f'{CODE}/b/merge_ddl_pred_data.do'
    log:
        f'{TMP}/push_pred_data.log'
    shell: f'source {CODE}/b/push_public_data.sh > {{log}}'

# creation of vector tileset from geojson
rule create_vector_tileset:
    input:
        rules.test_merged_data.log,        
        rules.dist_data_to_geojson.output,
        rules.map_data_to_geojson.output,
        f'{CODE}/b/create_vector_tileset.sh'
    output: f'{TMP}/covid_data.mbtiles'
    shell: 'source {CODE}/b/create_vector_tileset.sh {rules.dist_data_to_geojson.output} {rules.map_data_to_geojson.output}'

# upload of mbtiles to mapbox studio
rule push_vector_tileset:
    input:
        rules.create_vector_tileset.output,
        f'{CODE}/b/push_vector_tileset.py'
    conda: 'config/forecasting.yaml'
    log: f'{TMP}/tileset_push.log'
    shell: f'python {CODE}/b/push_vector_tileset.py --file {rules.create_vector_tileset.output} --token {MBTOKEN_TILE} --tilesetname {TILESET_NAME} > {{log}}'

           

############
# COMMANDS #
############

# running
#snakemake --cores 4 --use-conda
#snakemake --cores 4

# dry run:
#snakemake -n

# run a single job
#snakemake -R --until pull_predicted_data --cores 4 --use-conda

# viewing DAG:
#snakemake --dag | dot -Tpdf > ~/public_html/png/dag.pdf
#snakemake --forceall --rulegraph | dot -Tpdf > ~/public_html/png/dag.pdf
# viewable here: https://caligari.dartmouth.edu/~lunt/png/dag.pdf

# Report
# note: snakemake --report ~/public_html/report.html
# viewable here: https://caligari.dartmouth.edu/~lunt/report.html

# running on Discovery cluster - SLURM
# depends on profile set in ~/.config/snakemake/slurm/config.yaml (contact Toby for help)
# snakemake --profile slurm
